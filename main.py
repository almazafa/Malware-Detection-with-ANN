import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
import seaborn as sns
from sklearn.preprocessing import LabelEncoder,MinMaxScaler
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import csv
import MySQLdb
from keras.layers import LSTM
#connecting to MySql
db = MySQLdb.connect(host="localhost",    # your host, usually localhost
                     user="root",         # your username
                     passwd="Aa123456",  # your password
                     db="fp")        # name of the data base
# you must create a Cursor object. It will let
#  you execute all the queries you need
cur = db.cursor()
# query
cur.execute("select * from main_tbl inner join tbl1 on main_tbl.file_id=tbl1.file_id inner join tbl2 on main_tbl.file_id=tbl2.file_id inner join tbl3 on main_tbl.file_id=tbl3.file_id inner join tbl4 on main_tbl.file_id=tbl4.file_id")
#writing the result to CSV
rows = cur.fetchall()
fp = open('C:\\Users\\project18\\PycharmProjects\\pythonProject\\Gil2csv', 'w')
myFile = csv.writer(fp,lineterminator='\n')
myFile.writerows(rows)
fp.close()
db.close()

#read the data + print shape
data=pd.read_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\Dataset.csv',header=None)
print('Shape of data: ' , data.shape)

label= data[0]
#find the unique elements of an array (pe-malicious, pe-legit)
labels=label.unique()
#drop column 0
data.drop(0,inplace=True,axis=1)
#printing pie that show our data
plt.pie(label.value_counts().values,labels=label.value_counts().index,shadow=True,autopct='%1.00f%%',explode=(0.1,0))
plt.show()

#Transform features by scaling each feature to a given range(0-1).
scaler=MinMaxScaler(feature_range=[0,1])
data=scaler.fit_transform(data)
#normalize labels with labelEncoder
lbl_enc=LabelEncoder()
label=lbl_enc.fit_transform(label)
label=to_categorical(label)
#Replace NaN values with zero
data_without_nan = np.nan_to_num(data)

#create new array to store pca result
drpca=[]
for n_components in range(1,data_without_nan.shape[1]):
    print('n_components = ',n_components)
    pca=PCA(n_components=n_components)
    pca.fit(data_without_nan)
    drpca.append(np.sum(pca.explained_variance_ratio_))

pca=PCA(n_components=0.95)
data_reduced=pca.fit_transform(data_without_nan)
np.sum(pca.explained_variance_ratio_)

plt.figure(figsize=(11,6))
plt.plot(range(n_components),drpca,'-s',color='b',markersize=2)
plt.plot([73,73],[0.3,0.95],'--',color='r')
plt.plot([0.95,73],[0.95,0.95],'--',color='r')
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
plt.xlim((0,486))

data_reduced.shape
data_reduced=pd.DataFrame(data_reduced)
corr=data_reduced.corr()
plt.figure(figsize=(20,24))
sns.heatmap(corr)
plt.show()

#Export data_reduced + label to csv => for DWH
df = pd.DataFrame(data_reduced)
df.to_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\PCA70.csv', index = False)
df1=pd.DataFrame(label)
df1.to_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\label.csv', index = False)

#import PCA70 from mysql
import MySQLdb
db = MySQLdb.connect(host="localhost",    # your host, usually localhost
                     user="root",         # your username
                     passwd="Aa123456",  # your password
                     db="finalprojectpca")        # name of the data base
cur = db.cursor()
cur.execute("select * from tbl1")
df2 = pd.DataFrame(cur.fetchall())
db.close()

newlabel=pd.read_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\newlabel.csv',header=None)
####
#model
X_train1,X_test1,Y_train1,Y_test1 = train_test_split(df2,newlabel,stratify=None,test_size=0.3,shuffle=False,random_state=42)
n_features=df2.shape[1]
n_steps=1
n_outputs=1
epochs=4
batch_size=128
verbose=1

#Performing Feature Scaling (normalization)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train1)
X_train1 =np.array(X_train1).reshape((X_train1.shape[0],n_steps,n_features))
#sc.transform(X_train)
X_test1 = np.array(X_test1).reshape((X_test1.shape[0]),n_steps,n_features)
#sc.transform(X_test)

import tensorflow as tf
from keras.models import Sequential

#Initialising RNN
rnn = tf.keras.models.Sequential()
rnn.add(tf.keras.layers.LSTM(250,activation='relu',return_sequences=True,input_shape=(n_steps,n_features)))
rnn.add(tf.keras.layers.LSTM(150,activation='relu',return_sequences=True,dropout=0.2))
rnn.add(tf.keras.layers.LSTM(150,activation='relu',return_sequences=True))
rnn.add(tf.keras.layers.Bidirectional(LSTM(50,activation='relu')))
rnn.add(tf.keras.layers.Dense(n_outputs,activation='sigmoid'))
rnn.compile(optimizer="adam",loss="binary_crossentropy",metrics=['accuracy'])
history_rnn=rnn.fit(X_train1,Y_train1,validation_data=(X_test1,Y_test1),batch_size=batch_size,epochs =epochs)
print(rnn.summary())

#printing Accuracy throw Numbere of epochs RNN
x=[i for i in range(1,epochs+1)]
plt.figure(figsize=(15,5))
plt.title("RNN")
plt.plot(x,history_rnn.history['accuracy'],label='accuracy')
plt.plot(x,history_rnn.history['val_accuracy'],label='val_accuracy')
plt.xlabel('Number of epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#printing Loss RNN
x=[i for i in range(1,epochs+1)]
plt.figure(figsize=(15,5))
plt.title("RNN")
plt.plot(x,history_rnn.history['loss'],label='loss')
plt.plot(x,history_rnn.history['val_loss'],label='val_loss')
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


#printing Confusion matrix RNN
from sklearn.metrics import confusion_matrix
y_pred1=rnn.predict(X_test1)
Y_pred1=np.argmax(y_pred1,axis=1)
y_test1=np.argmax(Y_test1,axis=1)
f_matrix1=confusion_matrix(Y_test1,y_pred1.round())
plt.title('RNN - confusion matrix')
sns.heatmap(f_matrix1/np.sum(f_matrix1), annot=True,fmt='.2%',xticklabels=labels[::-1],yticklabels=labels[::-1],cmap='Blues')
plt.show()

# Accuracy
from sklearn.metrics import accuracy_score
y_pred1=rnn.predict(X_test1)
accuracy_score(Y_test1, y_pred1.round(), normalize=True)

#####################ANN######################
newlabel=pd.read_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\newlabel.csv',header=None)

#model
X_train,X_test,Y_train,Y_test = train_test_split(df2,newlabel,stratify=None,test_size=0.3,shuffle=False,random_state=42)
n_features=df2.shape[1]
n_steps=1
n_outputs=1
epochs=4
batch_size=128
verbose=1

#label.shape[1]
#1#data_reduced.shape[1]
#
#Performing Feature Scaling (normalization)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train)
X_train = sc.transform(X_train)
X_test = sc.transform(X_test)
import tensorflow as tf
from keras.models import Sequential

#Initialising ANN
ann = tf.keras.models.Sequential()
#Adding First Hidden Layer, ReLU helps to prevent the exponential growth in
# the computation required to operate the neural network.
ann.add(tf.keras.layers.Dense(units=256,activation="relu",input_shape=(73,)))
#Adding Second Hidden Layer
ann.add(tf.keras.layers.Dense(units=256,activation="relu"))
#Adding Output Layer
ann.add(tf.keras.layers.Dense(units=n_outputs,activation="sigmoid"))
#Compiling ANN
ann.compile(optimizer="adam",loss="binary_crossentropy",metrics=['accuracy'],run_eagerly=True)
#Fitting ANN
history_ann=ann.fit(X_train,Y_train,validation_data=(X_test,Y_test),batch_size=batch_size,epochs =epochs)
#print summary
print(ann.summary())

#printing Accuracy throw Numbere of epochs
x=[i for i in range(1,epochs+1)]
plt.figure(figsize=(15,5))
plt.title("ANN")
plt.plot(x,history_ann.history['accuracy'],label='accuracy')
plt.plot(x,history_ann.history['val_accuracy'],label='val_accuracy')
plt.xlabel('Number of epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


#printing Loss
x=[i for i in range(1,epochs+1)]
plt.figure(figsize=(15,5))
plt.title("ANN - val_loss")
plt.plot(x,history_ann.history['loss'],label='loss')
plt.plot(x,history_ann.history['val_loss'],label='val_loss')
plt.xlabel('Number of epochs')
plt.ylabel('val_loss')
plt.legend()
plt.show()

#printing Confusion matrix ANN
from sklearn.metrics import confusion_matrix
y_pred=ann.predict(X_test)
Y_pred=np.argmax(y_pred,axis=1)
y_test=np.argmax(Y_test,axis=1)
f_matrix=confusion_matrix(Y_test,y_pred.round())
plt.title('ANN - confusion matrix',weight='bold')
sns.heatmap(f_matrix/np.sum(f_matrix), annot=True,fmt='.2%',xticklabels=labels[::-1],yticklabels=labels[::-1],cmap='Blues')
plt.xlabel("Predicted",weight='bold')
plt.ylabel("Actual",weight='bold')
plt.show()

y_pred=ann.predict(X_test)


# Confusion Matrix
from sklearn.metrics import confusion_matrix
y_pred=ann.predict(X_test)
confusion_matrix(Y_test, y_pred.round())

tn=2563
tp1=20396
fn=596
fp=562

specificity=tn/(tn+fp)
print(specificity)
sensitivity=tp1/(tp1+fn)
print(sensitivity)
f1score=2/(1/(tp1/(tp1+fp))+1/(tp1/(tp1+fn)))
print(f1score)
# Accuracy
from sklearn.metrics import accuracy_score
y_pred=ann.predict(X_test)
accuracy_score(Y_test, y_pred.round(), normalize=True)

# Recall
from sklearn.metrics import recall_score
recall_score(Y_test, y_pred.round(), average='binary')
# Precision
from sklearn.metrics import precision_score
precision_score(Y_test, y_pred.round(), average=None)

from sklearn.metrics import f1_score
f1_score(Y_test,y_pred.round(),average='binary')


#ANN vs RNN Accuracy
x=[i for i in range(1,epochs+1)]
plt.figure(figsize=(15,5))
plt.title("Accuracy: ANN vs RNN")
plt.plot(x,history_ann.history['accuracy'],label='ANN')
plt.plot(x,history_rnn.history['accuracy'],label='RNN')
plt.xlabel('Number of epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#ANN vs RNN loss
x=[i for i in range(1,epochs+1)]
plt.figure(figsize=(15,5))
plt.title("Loss: ANN vs RNN")
plt.plot(x,history_ann.history['loss'],label='ANN')
plt.plot(x,history_rnn.history['loss'],label='RNN')
plt.xlabel('Number of epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()



df5=pd.DataFrame(Y_test)
df5.to_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\Y_test.csv', index = False,header=False)
df6=pd.DataFrame(y_pred.round())
df6.to_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\Y_pred.csv', index = False,header=False)

df8=pd.DataFrame(X_test)
df8.to_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\X_test.csv', index = False,header=False)
####################################################


from tensorflow.keras.models import load_model
ann.save('mymodel3.h5')
newmodel=load_model('C:\\Users\\project18\\PycharmProjects\\pythonProject\\mymodel3.h5')
newmodel.summary()
#########################################################





newxtest=pd.read_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\newlabel.csv',header=None)

# Loading model to compare the results
data_check=pd.read_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\upload\\malware_-_1.csv',header=None)
len(data_check.columns)
len(data_check.index)
abc=newmodel.predict(data_check)
print(abc)
newmodel.predict(data_check)
print(newmodel.predict(data_check))
print(data_check[1])



dfxtest=pd.DataFrame(X_test)
dfxtest.to_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\X_test.csv', index = False)

dfytest=pd.DataFrame(Y_test)
dfytest.to_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\Y_test.csv', index = False,header=None)

dfytest2=pd.DataFrame(Y_train)
dfytest2.to_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\Y_train.csv', index = False,header=None)

##check 20 rows from pca30
data_check1=pd.read_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\20.csv',header=None)
fisg=ann.predict(data_check1)
##check 2-legit 2-mel
data_check=pd.read_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\5.csv',header=None)
ann.predict(data_check)

##check malicious1
data_check1=pd.read_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\check1.csv',header=None)
ann_check1=ann.predict(data_check1)
print(ann_check1)

##check malicious2
data_check2=pd.read_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\check2.csv',header=None)
ann_check2=ann.predict(data_check2)
print(ann_check2)

##check legit
data_check3=pd.read_csv('C:\\Users\\project18\\PycharmProjects\\pythonProject\\check8260 - legit.csv',header=None)
ann_check3=(ann.predict(data_check3))
print(ann_check3)
